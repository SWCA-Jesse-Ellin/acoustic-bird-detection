{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xmlp9lP3CnT7"
   },
   "source": [
    "## Tutorial: Accurate Bioacoustic Species Detection from Small Numbers of Training Clips Using the Biophony Model\n",
    "\n",
    "<br>\n",
    "<div style=\"width: 400px\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/02/Araripe_Manakin_%281%29.jpg\" width=\"850px\" />\n",
    "    <a href=\"https://commons.wikimedia.org/wiki/File:Araripe_Manakin_(1).jpg\" title=\"via Wikimedia Commons\">Rick elis.simpson</a> / <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA</a>\n",
    "</div>\n",
    "</br>\n",
    "\n",
    "The Araripe Manakin (<i>Antilophia bokermanni</i>) is one of the rarest birds in the world, found only in a tiny forested valley in Brazil at the base of the Araripe Plateau. This critically endangered species, first discovered in 1996, was pushed to the brink of extinction through habitat destruction as settlements encroach on the last remaining forested areas [(for more info)](http://datazone.birdlife.org/species/factsheet/araripe-manakin-antilophia-bokermanni). AI-fueled automatic bird call recognition (bioacoustic monitoring) is an important new tool for locating and restoring the health of such elusive species populations, but the accuracy of such systems inherently suffers from a lack of sufficient audio examples of the species to learn from.  \n",
    "\n",
    "[Conservation Metrics](http://conservationmetrics.com/), in collaboration with [Microsoft AI for Earth](http://aka.ms/aiforearth), is developing tools for accelerating bioacoustic monitoring with AI. This repository hosts some of those tools and demonstrates their application to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAcwo4TVAzzu"
   },
   "source": [
    "# **Introduction**\n",
    "This tutorial demonstrates how to quickly develop an automated system to detect the sounds of a species of interest using only a handful of short clips exemplifying the species.  For this tutorial we will build a detector for an endangered bird called the [Araripe Manakin](https://www.iucnredlist.org/species/22728410/23753034). The approach gets a leg up from the provided \"starter kit\" for the detection of complex species sounds. This starter kit employs Conservation Metrics' (CMI) Biophony Model, which encapsulates generalized bio-acoustic features extracted from hundreds of thousands of audio examples of more than 300 species by CMI, using techniques adapted from the computational field of Deep Learning.\n",
    "\n",
    "This tutorial starts from scratch walking the reader through how to prepare audio data in order to train a powerful but lightweight detector, and then to deploy the detector to detect new, previously unseen examples of the species (known as *inference*).\n",
    "\n",
    "Using only the Biophony Model and the open source `Python` toolkit `scikit-learn`, this tutorial trains in a matter of seconds a lightweight signal classifier called a Support Vector Machine (SVM). The resulting system is able detect the presence of 91% of the Araripe Manakin calls in previously unheard two-second clips, a relative improvement of greater than 76.9% over what is possible without the assistance of the Biophony Model.\n",
    "\n",
    "## Araripe Manakin *Antilophia bokermanni*\n",
    "The Araripe Manakin (*Antilophia bokermanni*) is a critically endangered bird from the family of manakins (Pipridae) and its population is thought to number only 800 individuals. It was discovered in 1996, scientifically described in 1998, and only found in a tiny area of forested valley at the base of the Araripe Plateau, Brazil. For more information please visit [Aquasis' website](http://aquasis.org/soldadinho/?lang=en), a local conservation organization working to protect the species and unique habitats it inhabits, and [American Bird Conservancy](https://abcbirds.org/save-the-araripe-manakin/#:~:text=In%202014%2C%20ABC%20helped%20Aquasis,Alliance%20for%20Zero%20Extinction%20species.&text=By%20donating%20today%2C%20you%20can,creeks%20found%20within%20are%20preserved). \n",
    "\n",
    "## Passive Acoustic Monitoring for Conservation\n",
    "Acoustic recordings are an important conservation tool, providing metrics of activity and relative abundance over space and time.  These metrics can inform land managers and wildlife biologist when making conservation decisions. Passive acoustic monitoring equipment such as [AudioMoths](https://www.openacousticdevices.info/), [BAR](https://frontierlabs.com.au/) and [SongMeters](https://www.wildlifeacoustics.com/products/song-meter-sm4) can collect thousands of hours of recordings, creating a need for efficient and effective techniques to process those data.\n",
    "\n",
    "## Getting Started\n",
    "The `Jupyter` notebook that accompanies this page can be found [here](https://github.com/microsoft/acoustic-bird-detection/blob/master/cmi-antbok.ipynb).\n",
    "\n",
    "If you want to train a detector with your own data, but do not have labeled data yet, a workflow for labeling acoustic events has been made available. An `R markdown` document with that workflow can be found [here](https://github.com/microsoft/acoustic-bird-detection/blob/master/labeling/Xenocanto_labeling_workflow_RNotebook.Rmd) and the rendered tutorial [here](https://github.com/microsoft/acoustic-bird-detection/blob/master/labeling/Xenocanto_labeling_workflow_RNotebook.md)\n",
    "\n",
    "We recommend that you start by cloning this repo, creating a virtual environment (either with `conda` or `venv`) and launch the `jupyter notebook` directly.\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Prepare your envrionment and launch the `Jupyter Notebook`\n",
    "\n",
    "```\n",
    "git clone git@github.com:microsoft/acoustic-bird-detection.git\n",
    "conda create -n birdsounds python=3.7\n",
    "conda activate birdsounds\n",
    "cd acoustic-bird-detection\n",
    "pip install -r requirements.txt\n",
    "jupyter notebook cmi-antbok.ipynb \n",
    "```\n",
    "\n",
    "# Download the Starter Kit\n",
    "\n",
    "The following code can be used to obtain the Biophony Model starter kit and import the necessary packages into the `Python` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below if you are running the notebook in a hosted environment\n",
    "# such as Google Colab to collect the libraries and supporting scripts you need.\n",
    "\n",
    "#! wget https://cmiforearth.blob.core.windows.net/cmi-antbok/download_resources.sh\\n\",\n",
    "#! wget https://cmiforearth.blob.core.windows.net/cmi-antbok/preprocess.py\\n\",\n",
    "#! pip install python_speech_features sklearn soundfile tensorflow\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W0dLzK6J3t1P",
    "outputId": "49e10769-3608-4fb9-a4ed-7bbf5fff9fa4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "random_seed = 1234\n",
    "\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import soundfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocess as cmi\n",
    "\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import model_from_config\n",
    "\n",
    "from python_speech_features import fbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f27Ps2iBqr00"
   },
   "source": [
    "# Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXCSfQsYAzz3"
   },
   "source": [
    "## Download Sample Data\n",
    " Get the Biophony Model, which is composed of a JSON file that defines a Keras model, and a file containing the pretrained weights for that model. \n",
    "Also get the data we will use here for SVM training and testing, composed of FLAC audio files, and a CSV file containing the time stamps of the  species sounds of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "0Nv-LlnD34yT",
    "outputId": "aedada19-b7e6-4f37-8645-44be20870424"
   },
   "outputs": [],
   "source": [
    "# Depending on your environment, you may need to modify this script,\n",
    "# which is located in the root of the repository.\n",
    "!sh download_resources.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYy0vmPNAzz7"
   },
   "source": [
    "## Data Preview\n",
    "\n",
    "If everything worked, we now have a directory structure in the root of the repository that looks like this:\n",
    "\n",
    "```\n",
    "resources/\n",
    "    cmi_mbam01.json # model definition\n",
    "    cmi_mbam01.h5 # model weights\n",
    "    ANTBOK_training_labels.csv # training labels\n",
    "    training/\n",
    "        clips/\n",
    "            Antilophia_bokermanni_118707.flac\n",
    "            ...\n",
    "```\n",
    "\n",
    "## Audio Preview\n",
    "\n",
    "The audio files in the `clips/` directory are each approximately 1 minute long. Later, we will split them into two-second clips for training and testing the models. \n",
    "\n",
    "For now, let's listen to one complete file. Use the `soundfile` library to read an example flac file.\n",
    "\n",
    "You can easily hear two Manakin calls in the first ten seconds of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "-5UVUUPCAzz8",
    "outputId": "03f2886d-0071-41ba-9658-18bbde6eeb12"
   },
   "outputs": [],
   "source": [
    "samples, fs = soundfile.read('resources/training/clips/Antilophia_bokermanni_118707.flac')\n",
    "print(\"file sample rate: {}\".format(fs))\n",
    "Audio(samples, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3d--FRkAzz_"
   },
   "source": [
    "## Spectrograms As Feature Data\n",
    "\n",
    "Spectrograms are a common way to visualize the frequency components of an audio signal over time. Here is a spectrogram of the first 10 seconds of the above audio file. Again, you should be able to clearly see Manakin calls at 2 seconds and 8 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "nvz2hezlAz0A",
    "outputId": "b8f850d6-fd06-488e-c139-57380d6be3f4"
   },
   "outputs": [],
   "source": [
    "freqs, times, sxx = spectrogram(samples[0:22050 * 10], fs=22050)\n",
    "# note that we have increased the contrast to highlight the calls.\n",
    "plt.pcolormesh(times, freqs, np.sqrt(sxx), vmax=0.005, shading=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iynyxs5nAz0D"
   },
   "source": [
    "### Mel-frequency spectrograms\n",
    "\n",
    "While the above image will look familiar if you have experience working with audio data, a more standard representation in audio recognition systems is a [Mel-frequency filter bank](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). This representation evens out the contributions of low and high frequencies in a way that benefits the automated detection of complex sounds.\n",
    "\n",
    "**We will use the mel filter bank energies computed on two-second audio clips as the training data for our models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "oYii8maJAz0G",
    "outputId": "7e56186b-864b-43ea-b447-1b07047393f4"
   },
   "outputs": [],
   "source": [
    "# visualize seconds 1-3 of the previous audio file, using Mel-frequency filter bank energies\n",
    "plt.pcolormesh(cmi.make_fbank(samples[22050 * 1:22050 * 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUDwfnSjAz0K"
   },
   "source": [
    "## Window Labels\n",
    "\n",
    "Now that we know how to turn audio data into feature data, we need to label each example as either a \"positive\" example if it contains a Manakin call or \"negative\" example if it does not.\n",
    "\n",
    "Take a look at `ANTBOK_training_labels.csv` and you will see each audio file annotated with the start and stop times of confirmed calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y39PIHfHAz0K",
    "outputId": "b13bb1b9-fea0-4fd0-b605-449e35eb77b4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"resources/ANTBOK_training_labels.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wkGXJfAAz0N"
   },
   "source": [
    "Our process for generating labels is to take the start and stop times for each call, round down to the nearest 2-second increment, and label those windows as a **1**. All other windows are labeled **0**. The details of this operation are in the file `preprocess.py`.\n",
    "\n",
    "We are left with two arrays. The first array is a nested structure that contains the raw audio data as well as the name of the source file for that clip, to help with debugging the models. The second array contains the label for each clip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "IKwwECdWrKl7",
    "outputId": "8cd5644e-c990-414d-dedf-91ca6f269c20"
   },
   "outputs": [],
   "source": [
    "data, labels = cmi.preprocess_data()\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoYhvM13C_zi"
   },
   "source": [
    "## Final Steps and Data Review\n",
    "### Train / Test Split, Filter Bank Creation\n",
    "\n",
    "The final step in our data preparation process is to split the data into *test* and *training* sets. The SVM will be trained from the training set, and evaluated on the held-out test set to evaluate how accurately the system will detect new sound examples not used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIwAUmKuAz0V"
   },
   "outputs": [],
   "source": [
    "# split train / test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=random_seed)\n",
    "\n",
    "# create mel-frequency energy spectrograms\n",
    "fbank_train = np.array([cmi.make_fbank(x) for x in X_train[:,0]])\n",
    "fbank_test = np.array([cmi.make_fbank(x) for x in X_test[:,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Review\n",
    "Now that we have about a thousand two-second windows, let's look at some positive and negative examples. These examples represent the \"ground truth\" that our models will try to learn from the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = 3\n",
    "plt.figure(figsize=(20, 10))\n",
    "for v in range(plots):\n",
    "    plt.subplot(2, plots, v + 1)\n",
    "    plt.pcolormesh(fbank_train[y_train == 1][v])\n",
    "    plt.title(\"positive\")\n",
    "    \n",
    "    plt.subplot(2, plots, v + 1 + plots)\n",
    "    plt.pcolormesh(fbank_train[y_train == 0][v])\n",
    "    plt.title(\"negative\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebPYZrV9Az0Y"
   },
   "source": [
    "# Model Building Part 1: PCA Features + SVM\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "A primary challenge in training a simple detector with a small number of examples is that bioacoustic spectrograms are large: In order to represent complex sounds with high enough fidelity, a large number of frequencies and time steps need to be represented. It is said that such inputs have *high dimensionality*, where the dimensionality is defined as the total number of time-frequency combinations. \n",
    "\n",
    "It is well established that simple detectors like SVMs are generally not able to learn with high accuracy from a small number of high-dimensional inputs; there are too few examples to learn how to extract the relevant information. For this reason, pre-processing steps are commonly employed that reduce the input dimensionality. The Biophony Model learns to do so while preserving information necessary for discriminating many different species sounds. Without the Biophony Model, generic techniques of dimensionality reduction must be applied, such as Principal Component Analysis (PCA).\n",
    "\n",
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is a common way of reducing the input dimensionality, while still preserving its relevant (or \"principal\") characteristics. We will first use PCA to decompose our mel-frequency spectrograms, and then train a Support Vector Machine on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOBXAqpxAz0Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# flatten our test and training sets into 1-dimensional vectors.\n",
    "X_flat_train = fbank_train.reshape(len(fbank_train), -1)\n",
    "X_flat_test = fbank_test.reshape(len(fbank_test), -1)\n",
    "\n",
    "# n_components is the size of the vector into which PCA will decompose its input. \n",
    "# This is another hyperparameter that can be tuned to get better accuracy.\n",
    "#\n",
    "# For our data, 100 was about the right number. \n",
    "#\n",
    "# Take a look at the scikit-learn documentation for more information about this.\n",
    "n_components = 100\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True, random_state=random_seed).fit(X_flat_train)\n",
    "\n",
    "# Now apply the PCA dimensionality reduction to the train and test spectrograms.\n",
    "X_train_pca = pca.transform(X_flat_train)\n",
    "X_test_pca = pca.transform(X_flat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82lZ-65HAz0b"
   },
   "source": [
    "## Support Vector Machine Training\n",
    "\n",
    "We now train a SVM on the PCA output. We use `scikit-learn` to do search over some of the SVM parameters to find the model that performs best on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "RedqczPuAz0b",
    "outputId": "7e5c0765-a3ef-422d-a450-da6a2a0182e7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 1e2, 1e3],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1.0, 10], }\n",
    "\n",
    "# Please read the scikit-learn document for information about\n",
    "# setting these hyperparameters.\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',\n",
    "                       random_state=random_seed),\n",
    "                   param_grid, cv=5)\n",
    "# Fit the SVM on our PCA data.\n",
    "svm = clf.fit(X_train_pca, y_train)\n",
    "print(svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "Kfkq523NAz0e",
    "outputId": "b17a8c93-3eab-403b-bda8-06ee7cd9ec28"
   },
   "outputs": [],
   "source": [
    "# Run the prediction step on our test data\n",
    "# and print the results.\n",
    "pca_pred = svm.predict(X_test_pca)\n",
    "print(classification_report(y_test, pca_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Rx83yyLAz0i"
   },
   "source": [
    "As is evident in the classification report, the best SVM fails to detect about half of the species sounds, reflected by the recall score of [0.61]. In visualizing some of the errors below, it is evident that the model misses some obvious cases, incorrectly predicting **0** (no species present) instead of **1** (species present).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "colab_type": "code",
    "id": "-lJGnMEcAz0i",
    "outputId": "31ea8a52-8b34-4ab3-9455-88112cca1ce2"
   },
   "outputs": [],
   "source": [
    "errors = y_test != pca_pred\n",
    "X_errors = X_test[errors]\n",
    "y_errors = y_test[errors]\n",
    "pca_errors = pca_pred[errors]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for k in range(8):\n",
    "    plt.subplot(2, 4, k + 1)\n",
    "    plt.pcolormesh(X_flat_test[errors][k].reshape([41,682]))\n",
    "    plt.title(\"actual: {} predicted: {}\\n{}\".format(y_errors[k],\n",
    "                                                  pca_errors[k],\n",
    "                                                  path.basename(X_errors[k][1])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXrdevYeAz0m"
   },
   "source": [
    "# Model Building Part 2: Transfer Learning from the Biophony Model\n",
    "\n",
    "## What Is Transfer Learning\n",
    "\n",
    "Training an accurate deep learning model for sound and image recognition from scratch is very often time and resource intensive, taking days or weeks even on powerful hardware. This assumes large volumes of labeled data are available for the training, which is often not the case.\n",
    "\n",
    "Transfer learning uses *pre-trained* models to accurately learn from a much smaller set of examples. Importantly, these examples can be specific to a *different* problem domain (like identifying Manakin calls) than the one on which the model was initially trained, so the model is effectively adapted for your task.\n",
    "\n",
    "The details of how transfer learning works are beyond the scope of this tutorial, but there are many great references and examples from different deep learning frameworks: \n",
    "- https://cs231n.github.io/transfer-learning/\n",
    "- https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "## Conservation Metrics Biophony Model\n",
    "\n",
    "In the introduction to this post, we discussed how Conservation Metrics built and trained a deep learning model for classifying several hundred species from acoustic data. We will now leverage that model to build an Araripe Manakin classifier from the data described in the previous sections.\n",
    "\n",
    "First we need to load the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "SOIJYtHCDXso",
    "outputId": "9e3f6dc5-eff1-430d-858b-5e7b1a7cec7a"
   },
   "outputs": [],
   "source": [
    "model = model_from_config(json.load(open('resources/cmi_mbam01.json', 'r')))\n",
    "model.load_weights('resources/cmi_mbam01.h5')\n",
    "\n",
    "# Let's look at the original BAM model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQl9mVkAAz0u"
   },
   "source": [
    "### Model Preparation\n",
    "\n",
    "\n",
    "The simplest form of transfer learning would be to retrain the last layer of the Biophony Model using the 'Keras' toolkit. Here instead, the last layer is completely replaced by a SVM trained in scikit-learn. In other words, the SVM is trained on the output of the second-to-last layer of the Biophony Model, which is said to be an *embedding* of the input audio spectrogram into a much lower dimensional 512 element-wide vector. We have found, like in other domains like face recognition, that this is generally a simpler and more accurate approach.\n",
    "\n",
    "'TensorFlow' makes this very easy. Use the [Functional API](https://keras.io/guides/functional_api/#extract-and-reuse-nodes-in-the-graph-of-layers) to save a new layer that is the output of the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8EEknvwAz0u"
   },
   "outputs": [],
   "source": [
    "feature_layers = [layer.output for layer in model.layers[:-4]]\n",
    "feature_model = tf.keras.Model(inputs=[model.input], outputs=feature_layers)\n",
    "\n",
    "# visualize the embedding model\n",
    "feature_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpMInxExAz0x"
   },
   "source": [
    "The new model `feature_model` takes the same Mel-frequency spectrograms we used previously as input, and decomposes them into an output feature vector. Like with the PCA example, we will use these output features to train a Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CGbb5YtWAz0x",
    "outputId": "8661c62b-5ca3-48d3-db48-ecdb15812b67"
   },
   "outputs": [],
   "source": [
    "# First we need to normalize the fbank input to be in line\n",
    "# with what the pre-trained model expects. [We scale and also\n",
    "# drop the final frequency bin]\n",
    "\n",
    "scale = 33.15998\n",
    "X_train_normal = fbank_train[:,:40,:] / scale\n",
    "X_test_normal = fbank_test[:,:40,:] / scale\n",
    "\n",
    "# reshape to batches for the embedding model.\n",
    "batch_train = X_train_normal.reshape(X_train_normal.shape[0],\n",
    "                                     X_train_normal.shape[1],\n",
    "                                     X_train_normal.shape[2],\n",
    "                                     1)\n",
    "\n",
    "batch_test = X_test_normal.reshape(X_test_normal.shape[0],\n",
    "                                   X_test_normal.shape[1],\n",
    "                                   X_test_normal.shape[2],\n",
    "                                   1)\n",
    "\n",
    "# embed the training and test data into 512 element feature vectors.\n",
    "# the last element in the list is the output for the final 512-wide layer.\n",
    "embeddings_train = feature_model(batch_train)[-1]\n",
    "embeddings_test = feature_model(batch_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "U3n8USPv0TBz",
    "outputId": "b7a5135f-079a-4daf-d3e1-94116ab7783e"
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 1e2, 1e3],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1.0, 10], }\n",
    "\n",
    "# Please read the scikit-learn document for information about\n",
    "# setting these hyperparameters.\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced',\n",
    "                       random_state=random_seed),\n",
    "                   param_grid, cv=5)\n",
    "# Fit the SVM on our embedding data.\n",
    "svm = clf.fit(embeddings_train.numpy(), y_train)\n",
    "print(svm.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o57feQB9Az02"
   },
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "sX9I7gzv0jXH",
    "outputId": "67f27323-58ce-4f42-cc0a-a2a43399ee34"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(embeddings_test.numpy())\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9ep3HWWAz06"
   },
   "source": [
    "The new SVM, with the benefit of the Biophony Model output in place of PCA, now misses only 9% of the species calls, down from 39%, an improvement of 76.9%. In visualizing the remaining errors, it is evident that many fewer of them are egregious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "colab_type": "code",
    "id": "uCz734RPRUcV",
    "outputId": "b113beb1-b89c-458f-8879-b284e0cdb365"
   },
   "outputs": [],
   "source": [
    "errors = y_test != y_pred\n",
    "X_errors = X_test[errors]\n",
    "y_errors = y_test[errors]\n",
    "emb_errors = y_pred[errors]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for k in range(4):\n",
    "    plt.subplot(2, 4, k + 1)\n",
    "    plt.pcolormesh(X_flat_test[errors][k].reshape([41,682]))\n",
    "    plt.title(\"actual: {} predicted: {}\\n{}\".format(y_errors[k],\n",
    "                                                  emb_errors[k],\n",
    "                                                  path.basename(X_errors[k][1])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcRyyKhuAz0-"
   },
   "source": [
    "# Summary\n",
    "In summary, we were able to quickly train a detector with a modest number of sound clips from a new signal, the song from the critically endangered Araripe manakin.  We then applied this detector to new data to predict the presence and absence of that sound in novel recordings.  Leveraging the Biophony model and transfer learning, we were able to improve recall by 76.9% while also improving precision. Acoustic recordings are an important conservation tool, providing metrics of activity and relative abundance over space and time.  These metrics can inform land managers and wildlife biologist when making conservation decisions. Passive acoustic monitoring equipment such as [AudioMoths](https://www.openacousticdevices.info/), [BAR](https://frontierlabs.com.au/), and [SongMeters](https://www.wildlifeacoustics.com/products/song-meter-sm4) can collect thousands of hours of recordings, creating a need for efficient and effective techniques to process those data.  This notebook provides the pieces needed to train and test a detection model and then apply it to new data, allowing the classification of these large acoustic datasets.  \n",
    "\n",
    "# Acknowledgments\n",
    "\n",
    "We would like to thank the individual contributors to this repository from Conservation Metrics: Abram Fleishman, [Chris Eberly](https://ceberly.github.io/about/), David Klein, and Matthew McKown.\n",
    "\n",
    "# Contributing\n",
    "\n",
    "This project welcomes contributions and suggestions.  Most contributions require you to agree to a\n",
    "Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\n",
    "the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n",
    "\n",
    "When you submit a pull request, a CLA bot will automatically determine whether you need to provide\n",
    "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\n",
    "provided by the bot. You will only need to do this once across all repos using our CLA.\n",
    "\n",
    "This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n",
    "For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\n",
    "contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cmi-antbok.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
